---
title: "Untitled"
author: "a"
date: "March 2023"
output:
  html_document:
    toc: True
    toc_float: True
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Let us start by loading the required packages for the project and
fixing the seed.

```{r, message = FALSE, results = FALSE}
require("tidyverse")    || is.null(install.packages("tidyverse"))    && require("tidyverse")
require("glmnet")       || is.null(install.packages("glmnet"))       && require("glmnet")
require("MASS")         || is.null(install.packages("MASS"))         && require("MASS")
require("scales")       || is.null(install.packages("scales"))       && require("scales")
require("testit")       || is.null(install.packages("testit"))       && require("testit")
require("fitdistrplus") || is.null(install.packages("fitdistrplus")) && require("fitdistrplus")
require("MCMCglmm")     || is.null(install.packages("MCMCglmm"))     && require("MCMCglmm")
require("monomvn")      || is.null(install.packages("monomvn"))      && require("monomvn")
require("patchwork")    || is.null(install.packages("patchwork"))    && require("patchwork")
set.seed(42)
```

We will also require a root mean square error function
to be used later when testing the different fits.

```{r}
rmse <- function(y_pred, y){ #not designed for readability
  assert("Different lengths for inputs!",
         length(y_pred) == length(y))
  (y_pred - y)             %>%
    (\(x) x / length(x))() |> 
    #· We are testing the limits of R ·#
    (\(x) x * x)()         |>
    sum()                  %>%  
    sqrt()                 
}
```

# Introduction

We have chosen a dataset which includes the housing prices for
a region, along with different factors.
The idea is to try and see whether we may predict the price based
on the rest of predictors.
Let us peek into the data:

```{r}
data <- read.csv("bayesian_regression_data.csv", sep = ";")
head(data, 5)
```

We will split our data into a train and test partition to compare
the different fits.

```{r}
p         <- 0.8
n         <- nrow(data)
train_idx <- sample(1:n, round(n*p), replace = F)

train     <- data[ train_idx, ]
test      <- data[-train_idx, ]
x_train   <- data[ train_idx, -16]
x_test    <- data[-train_idx, -16]
y_train   <- data[ train_idx,  16]
y_test    <- data[-train_idx,  16] 
```

Lastly, because we will later use methods like Lasso, we already
prepare a scaled train data set for later.

```{r}
sc   <- scale(train)
x_sc <- sc[, -16]
y_sc <- sc[,  16]
```

# Frequentist approach

We start with the frequentist approach.
First, we fit a model in which we use all of the variables as
predictors

```{r}
fit_full_frequentist  <- lm(SalePrice ~ ., data = train)
pred_full_frequentist <- predict(fit_full_frequentist, x_test)
summary(fit_full_frequentist)
```

We observe that there are several non-important variables.
Out target is to get rid of them and use a smaller selection
of predictors using Lasso with the one standard error rule
to find parameter.

```{r}
optimal_lambda <- 0.0

for (i in 1:10) {
    cv_lasso       <- cv.glmnet(x_sc, y_sc, alpha = 1, nfolds = 10)
    optimal_lambda <- optimal_lambda + cv_lasso$lambda.1se #1SE rule
}

optimal_lambda   <- optimal_lambda / 10
mod_lasso        <- glmnet(x_sc, y_sc, alpha = 1, lambda = optimal_lambda)
selected_idx     <- which(mod_lasso$beta != 0)
selected_vars    <- rownames(mod_lasso$beta)[selected_idx]

lhs_formula_fit  <- "SalePrice ~ "
rhs_formula_fit  <- paste(selected_vars, collapse = " + ")
formula_fit      <- as.formula(paste(lhs_formula_fit, rhs_formula_fit))
```

The predictors considered by Lasso are the following ones:

```{r}
selected_vars
```

Now we can repeat the fit but only using the predictors
considered by Lasso.

```{r}
fit_lasso_frequentist  <- lm(formula_fit, data = train)
pred_lasso_frequentist <- predict(fit_lasso_frequentist, x_test)
summary(fit_lasso_frequentist)
```

Another approach is to include and exclude variables sequentially
and, using the Bayesian Information Criteria (BIC) select the best
model, as the one that minimizes the BIC.
Once again we start with all variables as predictors.

```{r}
fit_step_frequentist  <- stepAIC(fit_full_frequentist,
                                 k = log(n), trace = FALSE)
pred_step_frequentist <- predict(fit_step_frequentist, x_test)
summary(fit_step_frequentist)
```

Now is time to compare the three models.
We will take the best as the "control" for the latter comparison
with Bayesian methods.

```{r}
(rmse_frequentist <- data.frame(
    full  = rmse(pred_full_frequentist,  y_test),
    lasso = rmse(pred_lasso_frequentist, y_test),
    step  = rmse(pred_step_frequentist,  y_test)
))
```

## Bayesian Approach

Now, for the bayesian approach, we are going to use the `MCMCglmm` function from the namesake package. To start with, we will use all variables and we will use a non-informative prior (default value already in the function, with $m=0$ and $V = 10^{10} \cdot I$)

```{r}
f <- formula(paste("SalePrice ~", paste(names(x_train), collapse = " + ")))
base_bayes_model <- MCMCglmm(f, data = train, verbose = FALSE)
bayesian_preds   <- predict(base_bayes_model, test)
summary(base_bayes_model)
```


Lets plot the posterior distribution of all the parameters
of the model, along with its traces and ACFs.

```{r}
traces    <- as.data.frame(base_bayes_model$Sol)
var_names <- c("(Intercept)", names(x_train))
for (v in var_names) {
    p1 <- traces[v] %>%
        ggplot(aes(x = as.numeric(row.names(traces)), y = .data[[v]])) +
            geom_line(col = "deeppink4") + xlab("MC iteration") + theme_bw()
    p2 <- traces[v] %>%
        ggplot(aes(x = .data[[v]])) + 
        geom_histogram(col = "white", fill = "dodgerblue3", bins = 30) + theme_bw()
    
    temp <- as.data.frame(acf(traces[v], plot = FALSE)$acf[-1, , 1 ])
    temp <- cbind(temp, as.numeric(rownames(temp)))
    colnames(temp) <- c("acf", "lag")
    
    p3 <- temp %>%
        ggplot(aes(x = lag, y = acf)) +
            geom_hline(aes(yintercept = 0)) +
            geom_segment(mapping = aes(xend = lag, yend = 0)) +
            geom_hline(aes(yintercept = 1.959964/sqrt(nrow(temp))),
                       linetype = 2, col = "deeppink4") +
            geom_hline(aes(yintercept = -1.959964/sqrt(nrow(temp))),
                       linetype = 2, col = "deeppink4") +
            xlab("Lag") + ylab("ACF") + theme_bw()
    
    print(p1 | p2 | p3)
}
```

We can compare the values of the parameters obtained with the frequentist and bayesian approach:

```{r}
f_vs_b <- cbind(fit_full_frequentist$coefficients, colMeans(base_bayes_model$Sol))
colnames(f_vs_b) <- c("Frequentist","Bayesian")
format(f_vs_b, scientific = 999)
```
The values are very similar to the ones obtained with the classical MLE.
This is because we have used non-informative priors.

And check the RMSR of the bayesian model:

```{r}
rmse(bayesian_preds,y_test)
```
It is also very similar to the ones obtained with classical MLE.

### Using a manual prior distibution

We will now try to set a manual prior distribution, with information that we assume to be relevant, and we will check if the model generated in this way gives us better results than with the previously used informative prior distribution.

#### Attempt 1

Using a prior $\mathcal{N}(0,\,1)$ for every 

```{r}
lambda    <- 1e-5
reg.prior <- list(
    B = list(mu = rep(0, length(x_train) + 1),
    V = diag(1 / lambda, length(x_train) + 1))
)

base_bayes_model_mprior <- MCMCglmm(f, prior = reg.prior,
                                           data = train, 
                                    verbose = FALSE)
bayesian_mprior_preds   <- predict(base_bayes_model_mprior, test)
summary(base_bayes_model_mprior)
rmse(bayesian_mprior_preds, y_test)
```

#### Attempt 2

Using a prior $\mathcal{N}(\mu_i,\,\sigma^2)$ 

- $\mu_i$: Mean of the variable $i$ in the training set.
- $\sigma^2$: 1/1e-5.

```{r}
lambda    <- 1e-5
reg.prior <- list(
    B = list(
      mu = c(0,unlist(lapply(x_train, mean), use.names = F)),
      V  = diag(c(1/lambda,sapply(x_train, var))))
)

base_bayes_model_mprior <- MCMCglmm(f, prior = reg.prior,
                                           data = train,
                                            verbose = FALSE)
bayesian_mprior_preds   <- predict(base_bayes_model_mprior, test)
summary(base_bayes_model_mprior)
rmse(bayesian_mprior_preds, y_test)
```

#### Attempt 3

Nos quedamos con este, banda

```{r}
lambda    <- 1e-5
reg.prior <- list(
    B = list(
      mu = c(0,ifelse(lapply(x_train, mean) > 0, 1,-1)),
      V  = diag(rep(1/lambda,length(x_train) + 1)))
)
base_bayes_model_mprior <- MCMCglmm(f, prior = reg.prior,
                                           data = train,
                                            verbose = FALSE)
bayesian_mprior_preds   <- predict(base_bayes_model_mprior, test)
summary(base_bayes_model_mprior)
rmse(bayesian_mprior_preds, y_test)
```


### Blasso

Now let's perform a Bayesian lasso (or BLASSO) to select variables:

```{r}
blasso_model <- blasso(x_train, y_train, verb=0)
```

Now, let's plot the frequency of the number of variables used:

```{r}
pb <- as.data.frame(blasso_model$m)

pl <- pb %>%
        ggplot(aes(x = as.numeric(rownames(pb)), y = blasso_model$m)) +
            geom_line(col="deeppink4") +
            xlab("MC iteration") + ylab("m") + theme_bw()

ph <- pb %>% 
    ggplot(aes(x = blasso_model$m)) + theme_bw() + xlab("m") +
    geom_histogram(col="white", fill="dodgerblue3", bins = 15)

pl | ph
```

It can be seen that the number of variables most often used is 13, although 12 and 14 are also used a large number of times.

The distribution of the values of the different coefficients can be seen in the following chart:

```{r}
bd <- as.data.frame(blasso_model$beta)
colnames(bd) <- names(x_train)
bd <- bd %>%
        pivot_longer(cols = names(bd), names_to = "Coefficient", values_to = "Values")

bd %>%
    ggplot(aes(x = Coefficient, y = Values)) +
    geom_boxplot() + theme_bw() +
    theme(axis.text.x = element_text(angle = 90))
```

We can see the probability that each coefficient of the regression is different from 0 with the following code:

```{r}
summary(blasso_model)$bn0
```
As we want to select variables, we will keep those variables for which the coefficient has a probability greater than 95% of being different from 0.

```{r}
(selected     <- names(x_train)[summary(blasso_model)$bn0 > 0.95])
```

And we generate a new Bayesian model using only these variables:

```{r}
lhs_formula  <- "SalePrice ~ "
rhs_formula  <- paste(selected, collapse = " + ")
full_formula <- as.formula(paste(lhs_formula, rhs_formula))
blasso_bayes_model <- MCMCglmm(full_formula, data = train, verbose = FALSE)
basso_preds        <- predict(blasso_bayes_model, test)
summary(blasso_bayes_model)
```


### RMSE  

To conclude this section, we will compare the RMSE of the different Bayesian models generated:

```{r}
rmse_bayesian <- data.frame(
    base       = rmse(bayesian_preds,  y_test),
    mprior     = rmse(bayesian_mprior_preds, y_test),
    blasso     = rmse(basso_preds, y_test)
)
rmse_bayesian
```

## Conclusion

```{r}
rmse_bayesian
rmse_frequentist
```


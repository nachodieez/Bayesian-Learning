---
title: "Bayesian Learning: Assignment 2"
author:
  - José Ignacio Díez Ruiz (100487766)
  - Carlos Roldán Piñero (100484904)
  - Pablo Vidal Fernández (100483812)
date: "March 2023"
output:
  html_document:
    toc: True
    toc_float: True
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Let us start by loading the required packages for the project and
fixing the seed.

```{r, message = FALSE, results = FALSE}
require("tidyverse")    || is.null(install.packages("tidyverse"))    && require("tidyverse")
require("glmnet")       || is.null(install.packages("glmnet"))       && require("glmnet")
require("MASS")         || is.null(install.packages("MASS"))         && require("MASS")
require("scales")       || is.null(install.packages("scales"))       && require("scales")
require("testit")       || is.null(install.packages("testit"))       && require("testit")
require("fitdistrplus") || is.null(install.packages("fitdistrplus")) && require("fitdistrplus")
require("MCMCglmm")     || is.null(install.packages("MCMCglmm"))     && require("MCMCglmm")
require("monomvn")      || is.null(install.packages("monomvn"))      && require("monomvn")
require("patchwork")    || is.null(install.packages("patchwork"))    && require("patchwork")
require("knitr")        || is.null(install.packages("knitr"))        && require("knitr")
require("kableExtra")   || is.null(install.packages("kableExtra"))   && require("knitr")

set.seed(42)
```

We will also require a root mean square error function
to be used later when testing the different fits.

```{r}
rmse <- function(y_pred, y){ #not designed for readability
  assert("Different lengths for inputs!",
         length(y_pred) == length(y))
  (y_pred - y)             %>%
    (\(x) x / length(x))() |> 
    #· We are testing the limits of R ·#
    (\(x) x * x)()         |>
    sum()                  %>%  
    sqrt()                 
}
```

# Introduction

We have chosen a dataset which includes the housing prices for
a region, along with different factors.
The idea is to try and see whether we may predict the price based
on the rest of predictors.
Let us peek into the data:

```{r}
data <- read.csv("bayesian_regression_data.csv", sep = ";")
head(data, 5) %>% kable()
```

We will split our data into a train and test partition to compare
the different fits.

```{r}
p         <- 0.8
n         <- nrow(data)
train_idx <- sample(1:n, round(n*p), replace = F)

train     <- data[ train_idx, ]
test      <- data[-train_idx, ]
x_train   <- data[ train_idx, -16]
x_test    <- data[-train_idx, -16]
y_train   <- data[ train_idx,  16]
y_test    <- data[-train_idx,  16] 
```

Lastly, because we will later use methods like Lasso, we already
prepare a scaled train data set for later.

```{r}
sc   <- scale(train)
x_sc <- sc[, -16]
y_sc <- sc[,  16]
```

# Frequentist Approach

We start with the frequentist approach.
First, we fit a model in which we use all of the variables as
predictors

```{r}
fit_full_frequentist  <- lm(SalePrice ~ ., data = train)
pred_full_frequentist <- predict(fit_full_frequentist, x_test)
summary(fit_full_frequentist)
```

We observe that there are several non-important variables.
Out target is to get rid of them and use a smaller selection
of predictors using Lasso with the one standard error rule
to find parameter.

```{r}
optimal_lambda <- 0.0

for (i in 1:10) {
    cv_lasso       <- cv.glmnet(x_sc, y_sc, alpha = 1, nfolds = 10)
    optimal_lambda <- optimal_lambda + cv_lasso$lambda.1se #1SE rule
}

optimal_lambda   <- optimal_lambda / 10
mod_lasso        <- glmnet(x_sc, y_sc, alpha = 1, lambda = optimal_lambda)
selected_idx     <- which(mod_lasso$beta != 0)
selected_vars    <- rownames(mod_lasso$beta)[selected_idx]

lhs_formula_fit  <- "SalePrice ~ "
rhs_formula_fit  <- paste(selected_vars, collapse = " + ")
formula_fit      <- as.formula(paste(lhs_formula_fit, rhs_formula_fit))
```

The predictors considered by Lasso are the following ones:

```{r}
selected_vars
```

Now we can repeat the fit but only using the predictors
considered by Lasso.

```{r}
fit_lasso_frequentist  <- lm(formula_fit, data = train)
pred_lasso_frequentist <- predict(fit_lasso_frequentist, x_test)
summary(fit_lasso_frequentist)
```

Another approach is to include and exclude variables sequentially
and, using the Bayesian Information Criteria (BIC) select the best
model, as the one that minimizes the BIC.
Once again we start with all variables as predictors.

```{r}
fit_step_frequentist  <- stepAIC(fit_full_frequentist,
                                 k = log(n), trace = FALSE)
pred_step_frequentist <- predict(fit_step_frequentist, x_test)
summary(fit_step_frequentist)
```

Now is time to compare the three models.
We will take the best as the "control" for the latter comparison
with Bayesian methods.

```{r}
(rmse_frequentist <- data.frame(
    full  = rmse(pred_full_frequentist,  y_test),
    lasso = rmse(pred_lasso_frequentist, y_test),
    step  = rmse(pred_step_frequentist,  y_test)
)) %>% kable()
```

# Bayesian Approach

Now, for the Bayesian approach, we are going to use the `MCMCglmm` function from the namesake package. To start with, we will use all variables and we will use a non-informative prior (default value already in the function, with $m=0$ and $V = 10^{10} \cdot I$)

```{r}
f <- formula(paste("SalePrice ~", paste(names(x_train), collapse = " + ")))
base_bayes_model <- MCMCglmm(f, data = train, verbose = FALSE)
bayesian_preds   <- predict(base_bayes_model, test)
summary(base_bayes_model) 
```


Let's plot the posterior distribution of all the parameters
of the model, along with its traces and ACFs.

```{r}
traces    <- as.data.frame(base_bayes_model$Sol)
var_names <- c("(Intercept)", names(x_train))
for (v in var_names) {
    p1 <- traces[v] %>%
        ggplot(aes(x = as.numeric(row.names(traces)), y = .data[[v]])) +
            geom_line(col = "deeppink4") + xlab("MC iteration") + theme_bw()
    p2 <- traces[v] %>%
        ggplot(aes(x = .data[[v]])) + 
        geom_histogram(col = "white", fill = "dodgerblue3", bins = 30) + theme_bw()
    
    temp <- as.data.frame(acf(traces[v], plot = FALSE)$acf[-1, , 1 ])
    temp <- cbind(temp, as.numeric(rownames(temp)))
    colnames(temp) <- c("acf", "lag")
    
    p3 <- temp %>%
        ggplot(aes(x = lag, y = acf)) +
            geom_hline(aes(yintercept = 0)) +
            geom_segment(mapping = aes(xend = lag, yend = 0)) +
            geom_hline(aes(yintercept = 1.959964/sqrt(nrow(temp))),
                       linetype = 2, col = "deeppink4") +
            geom_hline(aes(yintercept = -1.959964/sqrt(nrow(temp))),
                       linetype = 2, col = "deeppink4") +
            xlab("Lag") + ylab("ACF") + theme_bw()
    
    print(p1 | p2 | p3)
}
```

We can compare the values of the parameters obtained with the frequentist and bayesian approach:

```{r}
f_vs_b <- cbind(fit_full_frequentist$coefficients, colMeans(base_bayes_model$Sol))
colnames(f_vs_b) <- c("Frequentist","Bayesian")
format(f_vs_b, scientific = 999) %>% kable()
```
The values are very similar to the ones obtained with the classical MLE.
This is because we have used non-informative priors.

And check the RMSE of the bayesian model:

```{r}
rmse(bayesian_preds,y_test)
```

It is also very similar to the ones obtained with classical MLE.

## Using a manual prior distibution

We will now try to set a manual prior distribution, with information that we assume to be relevant, and we will check if the model generated in this way gives us better results than with the previously used informative prior distribution.


We will use a prior $\mathcal{N}(\mu_i,\,\frac{1}{\lambda})$ for every parameter
where:

- $\mu_i$ equals $1$ if the mean of the variable $i$ is $>0$ and $-1$ otherwise. 
- $\lambda = \frac{1}{10^5}$

Setting the lambda and prior distribution values:

```{r}
lambda    <- 1e-5
reg.prior <- list(
    B = list(
      mu = c(0,ifelse(lapply(x_train, mean) > 0, 1,-1)),
      V  = diag(rep(1/lambda,length(x_train) + 1)))
)
```

Next, we train the model and make the predictions:

```{r}
base_bayes_model_mprior <- MCMCglmm(f, prior = reg.prior,
                                       data = train,
                                       verbose = FALSE)
bayesian_mprior_preds   <- predict(base_bayes_model_mprior, test)
```

In the following chunk, the summary of the model can be seen:

```{r}
summary(base_bayes_model_mprior)
```

Finally, let's compute the RMSE for the model:

```{r}
rmse(bayesian_mprior_preds, y_test)
```

It is worse than for the model with uninformative prior distibution. 
Probably, the a priori distribution that we have provided is not good enough and it
is necessary to be an expert in the field in which you are working to be able to
provide an informative prior distribution.

## Blasso

Next, let's perform a Bayesian lasso (or BLASSO) to perform the variable selection:

```{r}
blasso_model <- blasso(x_train, y_train, verb=0)
```

After that, let's plot the frequency of the number of variables used:

```{r}
pb <- as.data.frame(blasso_model$m)

pl <- pb %>%
        ggplot(aes(x = as.numeric(rownames(pb)), y = blasso_model$m)) +
            geom_line(col="deeppink4") +
            xlab("MC iteration") + ylab("m") + theme_bw()

ph <- pb %>% 
    ggplot(aes(x = blasso_model$m)) + theme_bw() + xlab("m") +
    geom_histogram(col="white", fill="dodgerblue3", bins = 15)

pl | ph
```

It can be seen that the number of variables most often used is 13, although 12 and 14 are also used a large number of times.

The distribution of the values of the different coefficients can be seen in the following chart:

```{r}
bd <- as.data.frame(blasso_model$beta)
colnames(bd) <- names(x_train)
bd <- bd %>%
        pivot_longer(cols = names(bd), names_to = "Coefficient", values_to = "Values")

bd %>%
    ggplot(aes(x = Coefficient, y = Values)) +
    geom_boxplot() + theme_bw() +
    theme(axis.text.x = element_text(angle = 90))
```

We can see the probability that each coefficient of the regression is different from 0 with the following code:

```{r}
summary(blasso_model)$bn0
```

As we want to select variables, we will keep those variables for which the coefficient has a probability greater than 95% of being different from 0.

```{r}
(selected <- names(x_train)[summary(blasso_model)$bn0 > 0.95])
```

And we generate a new Bayesian model using only these variables.
First, we need to create the formula:

```{r}
lhs_formula  <- "SalePrice ~ "
rhs_formula  <- paste(selected, collapse = " + ")
full_formula <- as.formula(paste(lhs_formula, rhs_formula))
```

With the formula already generated, we can train the model and make the predictions.

```{r}
blasso_bayes_model <- MCMCglmm(full_formula, data = train, verbose = FALSE)
basso_preds        <- predict(blasso_bayes_model, test)
```

The summary of the model can be seen in the following chunk:

```{r}
summary(blasso_bayes_model)
```

To finish with BLASSO, we are going to compute the RMSE for the model.

```{r}
rmse(basso_preds, y_test)
```


## RMSE  

In order to conclude this section, we will compare the RMSE of the different Bayesian models generated:

```{r}
rmse_bayesian <- data.frame(
    base       = rmse(bayesian_preds, y_test),
    blasso     = rmse(basso_preds, y_test),
    mprior     = rmse(bayesian_mprior_preds, y_test)
    )

rmse_bayesian %>% kable()
```


The best results have been achieved with the base model, being somewhat worse the model
generated with BLASSO and the model manually selecting the a priori distribution.

# Conclusion

To give the final conclusion, let us first recall the RMSE values for all the
models we have generated throughout this assignment.

```{r}
rmses <- data.frame(
    freq_full    = rmse(pred_full_frequentist,  y_test),
    freq_lasso   = rmse(pred_lasso_frequentist, y_test),
    freq_step    = rmse(pred_step_frequentist,  y_test),
    bayes_base   = rmse(bayesian_preds, y_test),
    bayes_blasso = rmse(basso_preds, y_test),
    bayes_mprior = rmse(bayesian_mprior_preds, y_test)
)

rmses %>% kable(
  caption = "RMSEs for the different models"
  )
```






The table shows that the model with which we have obtained the best results is the
frequentist lasso model. We also believe it is important to highlight that the worst
frequentist model (full) gave better results than the best Bayesian model (base, i.e.
with an a priori non-informative distribution). As mentioned above, this model is
practically equivalent to the full frequentist model.

As a final remark, we have not obtained optimal results for our problem using Bayesian
methods, but if we had more information about the possible distribution of the
parameters, it is likely that we would have obtained better results with the Bayesian
approach than with the frequentist approach.





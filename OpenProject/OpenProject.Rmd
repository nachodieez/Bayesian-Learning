---
title: "Open Project"
author: 
  -   "José Ignacio Díez Ruiz (100487766)"
  -   "Carlos Roldán Piñero (100484904)"
  -   "Pablo Vidal Fernández (100483812)"
date: "`r Sys.Date()`"
output:
    html_document:
        toc: true
        toc_float: yes
        theme: cosmo
        code_folding: "hide"        
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Package loading

```{r, results=FALSE}
i.p <- (\(x) is.null(install.packages(x, repos = "https://cran.rediris.es")))
require(tidyverse)   || i.p("tidyverse")   && require(tidyverse)
require(lubridate)   || i.p("lubridate")   && require(lubridate)
require(fpp3)        || i.p("fpp3")        && require(fpp3)
require(fable)       || i.p("fable")       && require(fable)
require(fabletools)  || i.p("fabletools")  && require(fabletools)
require(patchwork)   || i.p("patchwork")   && require(patchwork)
require(tidymodels)  || i.p("tidymodels")  && require(tidymodels)
require(bayesmodels) || i.p("bayesmodels") && require(bayesmodels)
require(modeltime)   || i.p("modeltime")   && require(modeltime)
require(timetk)      || i.p("timetk")      && require(timetk)
require(bsts)        || i.p("bsts")        && require(bsts)
require(dlm)         || i.p("dlm")         && require(dlm)


# Plot theme
theme_set(theme_minimal())
# Plot functions
pretty_ts  <- function(.data, .var, title, color, lag_max = NULL) {
    norm <- sqrt(nrow(.data))
    p1   <- .data %>%
        autoplot(.data[[.var]], color = color) +
            labs(title = title, x = "", y = "")
    p2   <- .data %>%
        ACF(.data[[.var]], lag_max = lag_max) %>% 
        ggplot(aes(x = lag, y = acf)) +
            geom_hline(aes(yintercept = 0)) +
            geom_segment(mapping = aes(xend = lag, yend = 0), color = color) +
            geom_hline(aes(yintercept =  1.96/norm),
                       linetype = 2, color = "#838B8B") +
            geom_hline(aes(yintercept = -1.96/norm),
                       linetype = 2, color = "#838B8B") +
            labs(title = "", x = "Lags [months]", y = "ACF")
    p3 <- .data %>%
        PACF(.data[[.var]], lag_max = lag_max) %>% 
        ggplot(aes(x = lag, y = pacf)) +
            geom_hline(aes(yintercept = 0)) +
            geom_segment(mapping = aes(xend = lag, yend = 0), color = color) +
            geom_hline(aes(yintercept =  1.96/norm),
                       linetype = 2, color = "#838B8B") +
            geom_hline(aes(yintercept = -1.96/norm),
                       linetype = 2, color = "#838B8B") +
            labs(title = "", x = "Lags [months]", y = "PACF")
    
    print(p1 / (p2 | p3))
}
```

# Data loading 

We begin by loading our data and converting it to the adequate format. We have our data both in `ts` and `tsibble` formats. 

```{r}
co2  <- read.table("https://www.esrl.noaa.gov/gmd/webdata/ccgg/trends/co2/co2_mm_mlo.txt")
temp <- read.csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv",
                 header = TRUE, skip = 1, sep = ",", na.strings = "***")
co2     <- ts(co2[,5], start = c(1958, 3), frequency = 12)
co2_ts  <- co2 %>% as_tsibble()
temp_ts <- ts(as.vector(t(temp[,2:13])), start = c(1880, 1), frequency = 12) %>% 
    as_tsibble() %>% drop_na()
temp    <- temp_ts %>% as.ts()
```

Once we have loaded the data, let's visualize the last 65 years:

# Data visualization

```{r}
temp_ts %>% 
    filter(year(index) > 1958) %>% 
    autoplot(color = "#8B008B") +
        labs(title = "Mean monthly anomaly temperature [°C]",
             x = "", y = "") + theme_minimal()
co2_ts %>% 
    autoplot(color = "#43CD80") +
        labs(title = "Mean monthly CO concentration [mg/m³]",
             x = "", y = "") + theme_minimal()
```

We can see in both plots that there is a clear positive trend. We can also wonder if there is a linear relationship between the variables:

```{r}
# Merge the data
data_ts     <- tsibble(temp_ts %>% filter((year(index) > 1958) & (year(index) < 2023)))
co2_vec     <- co2_ts %>% filter((year(index) > 1958) & (year(index) < 2023))
data_ts$co2 <- co2_vec$value
colnames(data_ts)[2] <- "temp"
co_temp_lm <- lm(temp ~ co2, data_ts)
summary(co_temp_lm)
```

We obtain a reasonable model, with a very high $R^2$ and with a significant coefficient for the CO2. 

# Frequentist approach

Now, we can try to model the CO2 series with an ARIMA model.

```{r}
# Only normal differentiation
data_ts %>% 
    mutate(dco2 = difference(co2)) %>%
    pretty_ts("dco2", title = "Differenced CO concentrations", color = "#43CD80")
# Seasonal + normal differentiation
data_ts %>% 
    mutate(dco2 = difference(difference(co2, 12))) %>%
    pretty_ts("dco2", title = "Diff Diff(12) CO concentrations", color = "#43CD80")
```

We can see in the ACF and the PACF of the differentiated series (which looks stationary) there is a significant spike at the first lag, followed by a rapid decrease towards zero. This suggests that the ARIMA(0,1,1) model. 
However, we would also like to explore a seasonal ARIMA. If we take a look into the differentiated series after the differentiation of order 12 (as the data is yearly), we firstly see that it look stationary, and we can see a spike in the lag 12 of the ACF, and spikes at lags 12 and 24 followed by exponential decay in the PACF. Thus, this suggest an ARIMA(0,1,1)(0,1,1)[12] or ARIMA(0,1,1)(0,1,1)[12]. 

We will compare the models, and the one that auto ARIMA will select, using a train and test partition.

```{r, warning=FALSE}
# Train-test
train_ts   <- data_ts %>% filter(year(index) <= 2017)
test_ts    <- data_ts %>% filter(year(index) >  2017)
# Train the models
models_co2 <- train_ts %>% 
    fabletools::model(
        manual = ARIMA(co2 ~ 1 + pdq(0, 1, 1) + PDQ(0, 0, 0)),
        season011 = ARIMA(co2 ~ 1 + pdq(0, 1, 1) + PDQ(0, 1, 1, period = 12)),
        auto   = ARIMA(co2),
        season012 = ARIMA(co2 ~ 1 + pdq(0, 1, 1) + PDQ(0, 1, 2)))
forecast_co2 <- models_co2 %>% fabletools::forecast(test_ts)
# Test them
forecast_co2 %>% fabletools::accuracy(data_ts)
# Visualize them
train_ts %>% 
    filter(year(index) > 2012) %>% 
    autoplot(co2) +
    autolayer(forecast_co2) +
    autolayer(test_ts, co2) +
        labs(title = "Test forecast on CO concentration [mg/m³]",
             x = "", y = "") + theme_minimal()
```

ARIMA(0,1,1)(0,1,2)[12] is the best model in all metrics. Looking at the predictions for the test set, we see that all the models except the not seasonal one have very similar forecasts.

For clarification, the model selected by auto ARIMA is:

```{r}
models_co2$auto
```

Plotting the forecast of the following five years:

```{r, warning=FALSE}
co2_forecast <- data_ts %>% 
    fabletools::model(ARIMA(co2 ~ 1 + pdq(0, 1, 1) + PDQ(0, 1, 2, period = 12))) %>% 
    fabletools::forecast(h = "5 years")
data_ts %>% 
    filter(year(index) > 2012) %>% 
    autoplot(co2) +
    autolayer(co2_forecast, color = "#43CD80") +
        labs(title = "5 year forecast on CO concentration [mg/m³]",
             x = "", y = "") + theme_minimal()
```

The results seem entirely reasonable, as do the confidence intervals, which are not too wide. 

Now, we will perform the same steps for the temperature:

```{r}
# Only normal differentiation
data_ts %>% 
    mutate(dtemp = difference(temp)) %>%
    pretty_ts("dtemp", title = "Differenced Temperature anomaly", color = "#8B008B")
# Seasonal + normal differentiation
data_ts %>% 
    mutate(dtemp = difference(difference(temp, 12))) %>%
    pretty_ts("dtemp", title = "Diff Diff(12) Temperature anomaly", color = "#8B008B")
```

In the first plot, the differentiated series looks stationary, there is a sinusoidal decay in the ACF and spikes at lag 1 and lag 3 in the PACF. This suggests an ARIMA(3,1,0) model, or  

Regarding the second plot, it also looks stationary. In the ACF there is a spike at lag 1 and 12, with the exact same pattern in the PACF. This suggests an ARIMA(0,1,1)(0,1,1)[12] model.

Comparing the models, and the one that auto ARIMA selects:

```{r, warning=FALSE}
# Train the models
models_temp <- train_ts %>% 
    fabletools::model(
        arima013 = ARIMA(temp ~ 1 + pdq(0, 1, 3) + PDQ(0, 0, 0)),
        arima310 = ARIMA(temp ~ 1 + pdq(3, 1, 0) + PDQ(0, 0, 0)),
        season = ARIMA(temp ~ 1 + pdq(0, 1, 1) + PDQ(0, 1, 1, period = 12)),
        auto   = ARIMA(temp))
forecast_temp <- models_temp %>% fabletools::forecast(test_ts)
# Test them
forecast_temp %>% fabletools::accuracy(data_ts)
# Visualize them
train_ts %>% 
    filter(year(index) > 2012) %>% 
    autoplot(temp) +
    autolayer(forecast_temp) +
    autolayer(test_ts, temp) +
        labs(title = "Test forecast on Temperature anomaly [°C]",
             x = "", y = "") + theme_minimal()
```

By a little the best is ARIMA(3,1,0) with no seasonal component. 

The selected model by auto ARIMA is:

```{r}
models_temp$auto
```

Looking at the forecast of the following five years of this model:

```{r}
temp_forecast <- data_ts %>% 
    fabletools::model(ARIMA(temp ~ 1 + pdq(0, 1, 3) + PDQ(0, 0, 0))) %>% 
    fabletools::forecast(h = "5 years")
data_ts %>% 
    filter(year(index) > 2012) %>% 
    autoplot(temp) +
    autolayer(temp_forecast, color = "#8B008B") +
        labs(title = "5 year forecast on Temperature anomaly [°C]",
             x = "", y = "") + theme_minimal()
```

We see that the forecast looks very linear with a positive slope. The confidence intervals, however, are very wide. 

Now that we have covered the univariate case, let's try modelling them jointly. We add a year ago to account for possible retarded effect.

```{r}
# Temp ~ time + co2 + co2 a year ago
temp_fit <- data_ts %>% 
    fabletools::model(arima_310 = ARIMA(temp ~ 1 + pdq(3, 1, 0) + PDQ(0, 0, 0) + co2 + lag(co2, 12))) 
# Forecast 5 years
temp_fit_forecast <- temp_fit %>% 
    fabletools::forecast(new_data(data_ts, n = 12*5) %>% mutate(co2 = co2_forecast$.mean))
# Plot the forecast
data_ts %>% 
    filter(year(index) > 2012) %>% 
    autoplot(temp) +
    autolayer(temp_fit_forecast, color = "#8B008B") +
        labs(title = "5 year forecast on Temperature anomaly [°C]",
             x = "", y = "") + theme_minimal()
```

Let's visually compare the two models: <font style="color:#43CD80">the one with CO</font> and <font style="color:#8B008B;">the one without CO</font>.

```{r}
data_ts %>% 
    filter(year(index) > 2012) %>% 
    autoplot(temp) +
    autolayer(temp_fit_forecast, color = "#43CD80", level = NULL, lwd = 1) +
    autolayer(temp_forecast, color = "#8B008B", level = NULL, lwd = 1) +
        labs(title = "5 year forecast on Temperature anomaly [°C]",
             x = "", y = "") + theme_minimal()
```

Once we include the CO, the predictions are a bit smaller than without it. However, the slope is positive in both of them.

# Bayesian approach

We will now try the Bayesian approach.

```{r, echo = F}
temp <- data_ts %>% 
  select(temp) %>% 
  as.ts(frequency = 12)

temp_ts <- temp %>% 
  as_tsibble() %>% 
  drop_na()

temps_ts2 <- temp_ts %>% 
    mutate(index = as.Date(index)) %>% 
    as_tibble()

t <- 1:nrow(temp_ts) + 1959
```

## Kalman filter with filter estimates of variances

The first model we are going to try is the Kalman filter with filter estimates of variances. 

```{r}
mod1_temp  <- dlmModPoly(order = 1)

# Estimate the filtered values of the state vector 

filt1 <- dlmFilter(temp, mod1_temp)
names(filt1)

# Plot time series and fit.

plot(temp, type="p", pch=16, xlab="Year", ylab="Depth",
     main="Temperature", ylim = c(-2,3))
lines(t, filt1$m[-1], col = "dodgerblue", lwd = 1.2)

# Obtain confidence intervals 

var <- dlmSvd2var(filt1$U.C, filt1$D.C)
sd <- sqrt(unlist(var))

upper_bound <- filt1$m[-1] + 2*sd[-1]
lower_bound <- filt1$m[-1] - 2*sd[-1]

lines(t, upper_bound, lty=3, col="red")
lines(t, lower_bound, lty=3, col="red")

# Estimate  predicted values of the state vectors

points(t, filt1$a, lty=2, col="green") # forecast
points(t, filt1$f, lty=2, col="magenta") # forecast
```

We can see that this model fails to capture the positive trend of the series. Now, we will try to change the values of V and W:

```{r}
# Using fixed values of variances. Do V and W matter?

mod2_temp  <- dlmModPoly(order=1, dV=10, dW=1)
filt2 <- dlmFilter(temp, mod2_temp )
mod3  <- dlmModPoly(order=1, dV=1, dW=10)
filt3 <- dlmFilter(temp, mod3)
plot(temp, type="p", xlab="Year", ylab="",
       main="Mean month anomaly temperature" )
lines(t, filt1$m[-1], col="red")
lines(t, filt2$m[-1], col="blue")
lines(t, filt3$m[-1], col="green")
```

All the models are very similar but suffer from the same problem as before.

## BSTS

Next, let's check the BSTS model. First, we introduce the model components.

```{r}
ss <- AddLocalLinearTrend(list(), temp)
ss <- AddSeasonal(ss, temp, nseasons = 12)
```

After that, we fit the model.

```{r}
model1_temp <- bsts(temp,
               state.specification = ss,
               niter = 1000)
```

And do multiple plots.

```{r}
plot(model1_temp)
plot(model1_temp, "components")  
#
# Out of sample prediction (for 2013 along with the previous 3 years)
# 
pred1 <- predict(model1_temp, horizon = 60)
plot(pred1, plot.original = 240)
```

We can see that this model is able to capture the positive trend, and make predictions accordingly. However, the confidence intervals are very wide, and thus they are not very informative. 

## SARIMA

Now, we will try to Bayesian version of ARIMA. We will use a training and a test partition, and we will compare it to a naive model.

We first need to make a function to create a dates vector:

```{r}
datseq <- function(t1, t2) { 
    seq(as.Date(paste0(t1,"01"), "%Y%m%d"), 
               as.Date(paste0(t2,"01"), "%Y%m%d"),by="month")
}

dates <- datseq("195901", "202212")
```

Next, let's define the spplits for cross validation.

```{r}
splits <- initial_time_split(temps_ts2, prop = 0.8)
splits <- temps_ts2 %>%
    time_series_split(date_var = index, initial = "59 years", assess="5 years")
```

Next, let's fit, test and plot the model.

```{r}
model_fit_arima_bayesian <- sarima_reg(non_seasonal_ar = 0,
                             non_seasonal_differences = 1,
                             non_seasonal_ma = 3,
                             pred_seed = 100) %>% ## ARIMA(0,1,3)
    set_engine(engine = "stan") %>%
    fit(value ~ index, data = training(splits))

model_fit_naive <- random_walk_reg(seasonal_random_walk = TRUE, seasonal_period = 12) %>%
    set_engine("stan") %>%
    fit(value ~ index + month(index), data = training(splits))

plot(model_fit_arima_bayesian$fit$models$model_1)

models_tbl <- modeltime_table(
    model_fit_arima_bayesian,
    model_fit_naive
)

calibration_tbl <- models_tbl %>%
    modeltime_calibrate(testing(splits))

calibration_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = temps_ts2[-(1:1500),]
    ) %>%
    plot_modeltime_forecast(
        .legend_max_width = 25, # For mobile screens
        .interactive      = F
    )

```

We can see in the posterior predict plot that the model captures most of the variability in the data.

Regarding the predictions, the model does exactly that. The Bayesian model is clearly better than the naive model.

# Conclusions

Bayesian models can be useful on a number of occasions. 
Throughout this work, we have been able to test their effectiveness.
However, not everything is as easy as it seems. 
In the R programming language, there is a lack of
libraries with Bayesian time series models, perhaps due to their low popularity.

However, we would like to highlight *bayesmodels*, which, thanks to the
power of its package, *modeltime*, allows us to easily create
Bayesian time series models, although it is still at an early stage and
has a lot of work to do.

In short, Bayesian time series can work very well under certain
circumstances. However, they are not very popular today, which means that
there is not much software available for their implementation.


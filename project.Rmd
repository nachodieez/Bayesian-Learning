---
title: "Untitled"
author: "a"
date: "March 2023"
output:
  html_document:
    toc: True
    toc_float: True
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Let us start by loading the required packages for the project and
fixing the seed.

```{r, message = FALSE, results = FALSE}
require("tidyverse")    || is.null(install.packages("tidyverse"))    && require("tidyverse")
require("glmnet")       || is.null(install.packages("glmnet"))       && require("glmnet")
require("MASS")         || is.null(install.packages("MASS"))         && require("MASS")
require("scales")       || is.null(install.packages("scales"))       && require("scales")
require("testit")       || is.null(install.packages("testit"))       && require("testit")
require("fitdistrplus") || is.null(install.packages("fitdistrplus")) && require("fitdistrplus")
require("MCMCglmm")     || is.null(install.packages("MCMCglmm"))     && require("MCMCglmm")
require("monomvn")      || is.null(install.packages("monomvn"))      && require("monomvn")
require("patchwork")    || is.null(install.packages("patchwork"))    && require("patchwork")
set.seed(100)
```

We will also require a root mean square error function
to be used later when testing the different fits.

```{r}
rmse <- function(y_pred, y){ #not designed for readability
  assert("Different lengths for inputs!",
         length(y_pred) == length(y))
  (y_pred - y)             %>%
    (\(x) x / length(x))() |> 
    #· We are testing the limits of R ·#
    (\(x) x * x)()         |>
    sum()                  %>%  
    sqrt()                 
}
```

# Introduction

We have chosen a dataset which includes the housing prices for
a region, along with different factors.
The idea is to try and see whether we may predict the price based
on the rest of predictors.
Let us peek into the data:

```{r}
data <- read.csv("bayesian_regression_data.csv", sep = ";")
head(data, 5)
```

We will split our data into a train and test partition to compare
the different fits.

```{r}
p <- 0.8
n <- nrow(data)
train_idx <- sample(1:n, round(n*p), replace = F)

train   <- data[ train_idx, ]
test    <- data[-train_idx, ]
x_train <- data[ train_idx, -16]
x_test  <- data[-train_idx, -16]
y_train <- data[ train_idx,  16]
y_test  <- data[-train_idx,  16] 
```

Lastly, because we will later use methods like Lasso, we already
prepare a scaled train data set for later.

```{r}
sc   <- scale(train)
x_sc <- sc[, -16]
y_sc <- sc[,  16]
```

# Frequentist approach

We start with the frequentist approach.
First, we fit a model in which we use all of the variables as
predictors

```{r}
fit_full_frequentist  <- lm(SalePrice ~ ., data = train)
pred_full_frequentist <- predict(fit_full_frequentist, x_test)
summary(fit_full_frequentist)
```

We observe that there are several non-important variables.
Out target is to get rid of them and use a smaller selection
of predictors using Lasso to find parameter.

```{r}
optimal_lambda <- 0.0

for (i in 1:10) {
    cv_lasso       <- cv.glmnet(x_sc, y_sc, alpha = 1, nfolds = 10)
    optimal_lambda <- optimal_lambda + cv_lasso$lambda.1se
}

optimal_lambda   <- optimal_lambda / 10
mod_lasso        <- glmnet(x_sc, y_sc, alpha = 1, lambda = optimal_lambda)
selected_idx     <- which(mod_lasso$beta != 0)
selected_vars    <- rownames(mod_lasso$beta)[selected_idx]

lhs_formula_fit  <- "SalePrice ~ "
rhs_formula_fit  <- paste(selected_vars, collapse = " + ")
formula_fit      <- as.formula(paste(lhs_formula_fit, rhs_formula_fit))
```

Now we can repeat the fit but only using the predictors
considered by Lasso.

```{r}
fit_lasso_frequentist  <- lm(formula_fit, data = train)
pred_lasso_frequentist <- predict(fit_lasso_frequentist, x_test)
summary(fit_lasso_frequentist)
```

Another approach is to include and exclude variables sequentially
and, using the Bayesian Information Criteria (BIC) select the best
model, as the one that minimizes the BIC.
Once again we start with all variables as predictors.

```{r}
fit_step_frequentist  <- stepAIC(fit_full_frequentist,
                                 k = log(n), trace = FALSE)
pred_step_frequentist <- predict(fit_step_frequentist, x_test)
summary(fit_step_frequentist)
```

Now is time to compare the three models.
We will take the best as the "control" for the latter comparison
with Bayesian methods.

```{r}
rmse_frequentist <- data.frame(
    full = rmse(pred_full_frequentist,  y_test),
    laso = rmse(pred_lasso_frequentist, y_test),
    step = rmse(pred_step_frequentist,  y_test)
)
rmse_frequentist
```

## Bayesian Approach


```{r}
f <- formula(paste("SalePrice ~", paste(names(x_train), collapse = " + ")))
base_bayes_model <- MCMCglmm(f, data = train, verbose = FALSE)
bayesian_preds   <- predict(base_bayes_model, test)
summary(base_bayes_model)
```


Lets plot the distribution of the parameters

```{r}
traces    <- as.data.frame(base_bayes_model$Sol)
var_names <- matrix(c("(Intercept)", names(x_train)), ncol = 4)
for (i in 1:4) {
    p11 <- traces[var_names[i, 1]] %>%
        ggplot(aes(x = as.numeric(row.names(traces)), y=.data[[var_names[i, 1]]])) +
            geom_line(col="deeppink4") + xlab("MC iteration") + theme_bw()
    p12 <- traces[var_names[i, 1]] %>%
        ggplot(aes(x=.data[[var_names[i, 1]]])) + 
        geom_histogram(col="white", fill="dodgerblue3", bins = 30) + theme_bw()
    
    p21 <- traces[var_names[i, 2]] %>%
        ggplot(aes(x = as.numeric(row.names(traces)), y=.data[[var_names[i, 2]]])) +
            geom_line(col="deeppink4") + xlab("MC iteration") + theme_bw()
    p22 <- traces[var_names[i, 2]] %>%
        ggplot(aes(x=.data[[var_names[i, 2]]])) + 
        geom_histogram(col="white", fill="dodgerblue3", bins = 30) + theme_bw()
    
    p31 <- traces[var_names[i, 3]] %>%
        ggplot(aes(x = as.numeric(row.names(traces)), y=.data[[var_names[i, 3]]])) +
            geom_line(col="deeppink4") + xlab("MC iteration") + theme_bw()
    p32 <- traces[var_names[i, 3]] %>%
        ggplot(aes(x=.data[[var_names[i, 3]]])) + 
        geom_histogram(col="white", fill="dodgerblue3", bins = 30) + theme_bw()
    
    p41 <- traces[var_names[i, 4]] %>%
        ggplot(aes(x = as.numeric(row.names(traces)), y=.data[[var_names[i, 4]]])) +
            geom_line(col="deeppink4") + xlab("MC iteration") + theme_bw()
    p42 <- traces[var_names[i, 4]] %>%
        ggplot(aes(x=.data[[var_names[i, 4]]])) + 
        geom_histogram(col="white", fill="dodgerblue3", bins = 30) + theme_bw()
    
    print((p11 | p12) / (p21 | p22) / (p31 | p32) / (p41 | p42))
}
```



```{r}
f_vs_b <- cbind(fit_full_frequentist$coefficients, colMeans(base_bayes_model$Sol))
colnames(f_vs_b) <- c("Frequentist","Bayesian")
format(f_vs_b, scientific = 999)
```

## Using a manual prior distibution

Using an uninformative prior distribution

```{r}
lambda    <- 1e-5
reg.prior <- list(
    B = list(mu = rep(0, length(x_train) + 1),
    V = diag(1 / lambda, length(x_train) + 1)))

base_bayes_model_uninformative <- MCMCglmm(f, prior = reg.prior,
                                           data = train, verbose = FALSE)
bayesian_uninformative_preds   <- predict(base_bayes_model_uninformative, test)
summary(base_bayes_model_uninformative)
```

### Blasso


```{r}
blasso_model <- blasso(x_train, y_train, verb=0)
selected     <- names(x_train)[summary(blasso_model)$bn0 > 0.95]
lhs_formula  <- "SalePrice ~ "
rhs_formula  <- paste(selected, collapse = " + ")
full_formula <- as.formula(paste(lhs_formula, rhs_formula))
blasso_bayes_model <- MCMCglmm(full_formula, data = train, verbose = FALSE)
basso_preds        <- predict(blasso_bayes_model, test)
summary(blasso_bayes_model)
```


```{r}
pb <- as.data.frame(blasso_model$m)

pl <- pb %>%
        ggplot(aes(x = as.numeric(rownames(pb)), y = blasso_model$m)) +
            geom_line(col="deeppink4") +
            xlab("MC iteration") + ylab("m") + theme_bw()

ph <- pb %>% 
    ggplot(aes(x = blasso_model$m)) + theme_bw() + xlab("m") +
    geom_histogram(col="white", fill="dodgerblue3", bins = 15)

pl | ph
```


```{r}
bd <- as.data.frame(blasso_model$beta)
colnames(bd) <- names(x_train)
bd <- bd %>%
        pivot_longer(cols = names(bd), names_to = "Coefficient", values_to = "Values")

bd %>%
    ggplot(aes(x = Coefficient, y = Values)) +
    geom_boxplot() + theme_bw() +
    theme(axis.text.x = element_text(angle = 90))
```


### RMSE  

```{r}
rmse_bayesian <- data.frame(
    base          = rmse(bayesian_preds,  y_test),
    uninformative = rmse(bayesian_uninformative_preds, y_test),
    blasso        = rmse(basso_preds, y_test)
)
rmse_bayesian
```
